{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KYR6wx4CQVvI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, Sequential, load_model\n",
        "#from keras.layers import Conv2D,MaxPooling2D,\\\n",
        "#     Dropout,Flatten,Dense,Activation,\\\n",
        "#     BatchNormalization\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import load_img\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3SbWNYM5c_U",
        "outputId": "7c12b56d-8cc6-44ba-cfdc-abee20c510de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "#load image data on collab\n",
        "!mkdir -p ML_Project/dataset\n",
        "!unzip -uq \"/content/drive/MyDrive/ML_Project/CatsDogs.zip\" -d \"/content/ML_Project/dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4UuhuozZWrF"
      },
      "source": [
        "\n",
        "##**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_PyB3EQscwJT"
      },
      "outputs": [],
      "source": [
        "#Params\n",
        "#Paths\n",
        "project_dir = \"/content/ML_Project\"\n",
        "data_dir = project_dir + \"/dataset/CatsDogs\"\n",
        "cats_dir = data_dir + '/Cats'\n",
        "dogs_dir = data_dir + '/Dogs'\n",
        "model_dir = '/content/drive/MyDrive/ML_Project/model/'\n",
        "plots_dir = '/content/drive/MyDrive/ML_Project/plots/'\n",
        "#Image params\n",
        "Image_Width=128\n",
        "Image_Height=128\n",
        "Image_Channels=3\n",
        "IMAGE_SHAPE = (Image_Width, Image_Height, Image_Channels)\n",
        "IMAGE_SIZE = (Image_Width, Image_Height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d4v0l1j84e4u"
      },
      "outputs": [],
      "source": [
        "#check for corrupt exif data \n",
        "def remove_exif(data_dir):\n",
        "  num = 0\n",
        "  failed = 0\n",
        "  for image_name in os.listdir(data_dir):\n",
        "    path = os.path.join(data_dir, image_name)\n",
        "    image = Image.open(path)\n",
        "    try:\n",
        "      if not image.getexif():\n",
        "        num+=1\n",
        "      data = list(image.getdata())\n",
        "      image_without_exif = Image.new(image.mode, image.size)\n",
        "      image_without_exif.putdata(data)\n",
        "      image_without_exif.save(path)\n",
        "    except:\n",
        "      failed+=1\n",
        "    return(print(f'Corrupt exif data where removed from {num} files. The process failed for {failed} files'))\n",
        "    \n",
        "\n",
        "#def downscale(Image_Size, img_path):\n",
        "#  #resize images\n",
        "#  for im in img_path:\n",
        "#    img_path = os.path.join(img_path, img)\n",
        "#    img = cv2.imread(img_path)\n",
        "#    img = cv2.resize(img, Image_Size, interpolation = cv2.INTER_AREA)\n",
        "#    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TfhjMNI66f5D"
      },
      "outputs": [],
      "source": [
        "#add class to image names to create unique image ids in each folder\n",
        "cats_filenames = ['0' + filename for filename in os.listdir(cats_dir)]\n",
        "dogs_filenames = ['1' + filename for filename in os.listdir(dogs_dir)]\n",
        "#rename and move images in same folder\n",
        "for cat, dog in zip(cats_filenames, dogs_filenames):\n",
        "  cat_src = cats_dir + '/' + cat[1:]\n",
        "  dog_src = dogs_dir + '/' + dog[1:]\n",
        "  cat_dst = data_dir + '/' + cat\n",
        "  dog_dst = data_dir + '/' + dog\n",
        "  shutil.move(cat_src, cat_dst)\n",
        "  shutil.move(dog_src, dog_dst)\n",
        "#now that dogs and cats have filename including class_id, we can remove original directories\n",
        "os.rmdir(cats_dir)\n",
        "os.rmdir(dogs_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqquymZhHhYD",
        "outputId": "7f8661ec-3fb4-495b-a561-337a784adcba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A total of 2 corrupted files were spotted and removed\n"
          ]
        }
      ],
      "source": [
        "#remove corrupted files\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "faulty_img = []\n",
        "for img in os.listdir(data_dir):\n",
        "  img_path = os.path.join(data_dir, img)\n",
        "  try:\n",
        "    img = Image.open(img_path)\n",
        "  except:\n",
        "    faulty_img.append(img_path)\n",
        "    os.remove(img_path)\n",
        "print(f'A total of {len(faulty_img)} corrupted files were spotted and removed')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check for corrput exif data\n",
        "remove_exif(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgQwLA4avpiQ",
        "outputId": "ca802346-cdd1-4b99-8845-8e7f34076b0f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrupt exif data where removed from 1 files. The process failed for 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove corrupt images\n",
        "skipped_files = []\n",
        "for filename in os.listdir(data_dir):\n",
        "  file_path = os.path.join(data_dir, filename)\n",
        "  try:\n",
        "    fobj = open(file_path, \"rb\")\n",
        "    is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "  finally:\n",
        "    fobj.close()\n",
        "  if not is_jfif:\n",
        "    skipped_files.append(file_path)\n",
        "    # Delete corrupted image\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(f\"Deleted {len(skipped_files)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnXwWgqYt2sZ",
        "outputId": "3642dc0e-a9cc-466a-98e3-089850e70caa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted 1588 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4_FjtgAuZ-0W"
      },
      "outputs": [],
      "source": [
        "#explore images\n",
        "file_size = []\n",
        "file_dim = []\n",
        "#img that throw error when opened in cv2 but work when fed to the data generator\n",
        "skipped_files = []\n",
        "#poor quality images\n",
        "low_res_img = []\n",
        "for img in os.listdir(data_dir):\n",
        "  #read image\n",
        "  img_path = os.path.join(data_dir, img)\n",
        "  im = cv2.imread(img_path)\n",
        "  try:\n",
        "    #divide along the three dimensions of the ndarray to get pixel count\n",
        "    size = im.size/3\n",
        "    #shape = (num_height_pxl, num_width_pxl)\n",
        "    shape = im.shape[:2]\n",
        "    #store information\n",
        "    if size<=50000:\n",
        "      low_res_img.append(img_path)\n",
        "    file_size.append(size)\n",
        "    file_dim.append(shape)\n",
        "  except:\n",
        "    skipped_files.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "38O0YXefpdXq",
        "outputId": "ca69352f-79ce-40a4-a356-b118f66ba765"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#plot num_pixels distribution of image datasets\n",
        "pxl_count = [el for el in file_size]\n",
        "#define range of pixels\n",
        "minimum = min(pxl_count)\n",
        "maximum = max(pxl_count)\n",
        "range = maximum-minimum\n",
        "frequency = 5000\n",
        "#histogram\n",
        "_,_,bins = plt.hist(pxl_count, color = 'grey', edgecolor = 'black',\n",
        "                    bins = int(range/frequency))\n",
        "for bin in bins:\n",
        "  if bin.get_x() <= int(Image_Height*Image_Width//frequency*frequency):\n",
        "    bin.set_facecolor('red')\n",
        "# Add labels\n",
        "plt.title('Histogram Pixel Counts')\n",
        "plt.xlabel('Number of Pixels')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.axvline(x=Image_Height*Image_Width, linestyle='--',linewidth=1, color='grey')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0SA9973MhW4s"
      },
      "outputs": [],
      "source": [
        "#visualize poor quality images\n",
        "bad_im_size = []\n",
        "bad_im_dim = [] \n",
        "for path in low_res_img:\n",
        "  im = cv2.imread(path)\n",
        "  size = im.size/3\n",
        "  shape = im.shape[:2]\n",
        "  bad_im_size.append(size)\n",
        "  bad_im_dim.append(shape)\n",
        "max_size = file_size.index(max(file_size))\n",
        "min_size = file_size.index(min(file_size))\n",
        "max_shape = file_dim[max_size]\n",
        "min_shape = file_dim[min_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "sQ71QYS8gzLQ",
        "outputId": "999bc8bb-0958-428f-9a51-b4df4e8f9aa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcea82779d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "worst_img_path = low_res_img[bad_im_dim.index(min_shape)]\n",
        "worst_img = cv2.imread(worst_img_path)\n",
        "plt.imshow(worst_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XvuygjE3fodT"
      },
      "outputs": [],
      "source": [
        "#remove worst image and corrupt data\n",
        "os.remove(worst_img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S5JKAAk_AJB3"
      },
      "outputs": [],
      "source": [
        "#associate class id and category to images \n",
        "all_filenames = os.listdir(data_dir)\n",
        "category = []\n",
        "class_id = []\n",
        "for el in all_filenames:\n",
        "    if el[0]=='1':\n",
        "        category.append('dog')\n",
        "        class_id.append('1')\n",
        "    elif el[0]=='0':\n",
        "        category.append('cat')\n",
        "        class_id.append('0')\n",
        "    else:\n",
        "        print(f'Anomalous filename in iteration {all_filenames.index(el)}')  \n",
        "\n",
        "#create df\n",
        "df = pd.DataFrame({\n",
        "    'filename': all_filenames,\n",
        "    'category': category,\n",
        "    'class_id': class_id\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3DWiAdkd8yfb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# create train and test split keeping ballanced classes\n",
        "X = df['filename']\n",
        "y = df['class_id']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
        "# get validation set from train set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42, stratify=y_train)\n",
        "'''\n",
        "# create train and test split keeping ballanced classes\n",
        "train_df, test_df = train_test_split(df[['filename', 'class_id']], test_size=0.10, random_state=42, stratify= df['class_id'])\n",
        "# get validation set from train set\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.20, random_state=42, stratify=train_df['class_id'])\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RhSY6Di_7QcC"
      },
      "outputs": [],
      "source": [
        "#move images to split set\n",
        "train_path = data_dir + '/train/'\n",
        "val_path = data_dir + '/val/'\n",
        "test_path = data_dir + '/test/'\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(val_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        "train = [shutil.move(data_dir + '/' + el, train_path + el) for el in list(train_df['filename'])]\n",
        "val = [shutil.move(data_dir + '/' + el, val_path + el) for el in list(val_df['filename'])]\n",
        "test = [shutil.move(data_dir + '/' + el, test_path + el) for el in list(test_df['filename'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MXqQfbhnJzET"
      },
      "outputs": [],
      "source": [
        "#add image absolute paths to filenames\n",
        "train_df['filename'] = train_path + train_df['filename'].astype(str)\n",
        "val_df['filename'] = val_path + val_df['filename'].astype(str)\n",
        "test_df['filename'] = test_path + test_df['filename'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ROa-GX_mF6Jq",
        "outputId": "697c8ac1-9330-4f74-d366-ae4f47759528"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#get class proportion in each set\\ntrain_cats_count = (y_train == '0').sum().sum()\\ntrain_dogs_count = (y_train == '1').sum().sum()\\nval_cats_count = (y_val == '0').sum().sum()\\nval_dogs_count = (y_val == '1').sum().sum()\\ntest_cats_count = (y_test == '0').sum().sum()\\ntest_dogs_count = (y_test == '1').sum().sum()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''\n",
        "#get class proportion in each set\n",
        "train_cats_count = (y_train == '0').sum().sum()\n",
        "train_dogs_count = (y_train == '1').sum().sum()\n",
        "val_cats_count = (y_val == '0').sum().sum()\n",
        "val_dogs_count = (y_val == '1').sum().sum()\n",
        "test_cats_count = (y_test == '0').sum().sum()\n",
        "test_dogs_count = (y_test == '1').sum().sum()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "jaGomIgNjSuu",
        "outputId": "ef7df7ad-4047-4095-cc99-1b991ec25370"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgV1bn2/+/NZCsgM7xCo2BAlEER2ykqL2oiDgk4EIMxCorhGI2Jkng0wzlqTKIxRo2/RD28xwGNkaBR4cREJERiyFFjo4iAiq1CoEUZxAGRofH5/VGr2w10Q7f07t7A/bmufXXVqlVVz97Vez97rVq7ShGBmZlZoWnS2AGYmZlVxwnKzMwKkhOUmZkVJCcoMzMrSE5QZmZWkJygzMysIDlB2S5N0kJJX2jsOLbXzvI88kHSPElDGjsOqzsnKPtMJH1NUqmk1ZKWSvqzpKNruW5I6rUd+5akNyTN/6zbqOV+7pG0XtKH6TFX0nWS2uRzvzXEsqekWyT9K73mr6f5jo0Qy2hJG1McH0iaLelLDR1HddIx+0luWUT0i4gZjRSSbQcnKKszSeOAW4CfAV2AvYHbgOENFMJgoDOwr6RD87yvGyKiNdAJOA84AviHpJZ53m8VSS2A6UA/4ERgT+BIYCVwWEPFsZmnI6IV0Ba4E5gkqd3mlSQ1a6iAJDVtqH1ZA4kIP/yo9QNoA6wGvrKVOocBTwPvAUuBXwMt0rKngAA+Stv5KtAR+GOq/y7wd6DJVrZ/F3A/8DDw682WzQCuBf4BfAg8AXTMWX4OsIjsw/2HwELgCzXs5x7gJ5uVtU7P6Vtpvgnwo7TNZcC9QJuc+ufm7O8/cveXXqdS4APgHeCmGuK4IC1vtZXXZPPt1vT6C7g5xfoB8BLQPy07GZifXrdy4Hs17Gs0MDNnvmU6piXA1cBDwG/T9i8AugJT0rEtA76Rs25l/d+n/T4PHJSz/IB0TN8D5gHDNjs+twN/Sv9PY4ENwHqy/63/qea12Y3sy9Vb6XELsFtaNgRYAnw3vT5LgfMa+z23Kz8aPQA/dqwH2Tf4CqDZVuocQtbSaAb0AF4GLs1ZHkCvnPnrgDuA5ulxDKAatr1H+uA7GTgDWFH54ZuWzwBeB/YDdk/z16dlfdMH1+D0QXVTei61TlCp/F7g92n6/PShuy/Qiixp3rfZ/o4GWgA3pg/Qyg/Lp4Fz0nQr4Iga4pgITNjGccn9EK7x9QeGArPIWj4iSwB7pWVLgWPSdDtgUA37Gk1KUGkf3yFLLm3IEs4G4FSy5L072ZeS24AiYCCwHDgurV9Zf0Q69t8D3sz5XygDfpBev+PSfvrkHJ/3gaPSvoqqO2abvTY/Bp4ha4F3Av4XuDYtG5L+H36c9n0ysAZo19jvu1314S4+q6sOwIqIqKipQkTMiohnIqIiIhYC/wX8361scwOwF7BPRGyIiL9H+sSoxunAOrKW0WNkHySnbFbn7ohYEBEfA5PIPhQh+xD8Y0Q8FRHryFo0n2ztydbgLaB9mj6brOXzRkSsBr4PjExdWyPIvsXPjIj1wH+SJefc591LUseIWB0Rz9Swvw5kyaNWtvH6byBrBe5P9iXg5YhYmrOsr6Q9I2JVRDy/ld0cIek94G3gLOC0iHg/LXs6Ih6NiE/IWsdHAVdExNqImA38N1nLstKsiHgoIjaQfWkoIkuwR5Al7usjYn1E/JWspX1WzrqTI+IfEfFJRKytxctzNvDjiFgWEcuBa8ha1ZU2pOUbIuJPZF8w+tRiu5YHTlBWVyuBjls7tyBpP0l/lPS2pA/IzlVt7WT+L8i+KT+RBj9cuZW6o4BJ6cN3LfCHVJbr7ZzpNWQfcpB1NS2uXBARH6XnU1fdyLqrKre5KGfZIrJWRZdq9rdms/2NIWvpvSLpua0MNFhJlsBrZWuvf/qQ/zXwG2CZpPGS9kyrnkHWalgk6W+SjtzKbp6JiLYR0TEijoiIv+QsW5wz3RV4NyI+zClbRPYablE/JbUlab2uwOJUts11a6m649U1Z37lZl++cv9/rIE5QVldPU3Wgjl1K3VuB14BekfEnmRdNKqpckR8GBHfjYh9gWHAOEnHb15PUjFZN8/X04fv22StlJNrOZptKdA9Z3t7kLVOak1SK+ALZOfJIGtN7ZNTZW+ybqJ30v6Kc9bdPXd/EfFaRJxF1t30c+ChGgZf/AUYWoeBGVt9/SPi1og4hKwLcj/g8lT+XEQMT/E8Stb6/CxyW4lvAe0ltc4p25vsHFel3GPShOw1qzxH1D2V1bTu5i3tbd2eobrj9dY21rFG4gRldZK6cf4T+I2kUyXtIam5pJMk3ZCqtSY7T7Ra0v7ANzfbzDtk52wAkPQlSb0kieycwkaq73o7B1hA1uUyMD32I/vGfVY19Tf3EPAlSUenkXE/ppbvAUm7STqE7IN7FXB3WvQAcJmknil5/Yzs/FRF2t+XJX0+7e9qchKFpK9L6pRaCO+l4uqe931kLYU/SNpfUhNJHST9QNLJ1dSv8fWXdKikwyU1JxtYsBb4RFILSWdLapO62j6oIZY6iYjFZOd5rpNUJOlAspbjb3OqHSLp9NQqv5TsC9AzwLNkLZh/T/9jQ4Avk52Tq8km/1vVeAD4kaRO6UvNf24WixUQJyirs4j4JTCObPTacrIPz2+RfXhDdqL7a2QntP8f2QitXFcDEyS9J+lMoDdZK2E1WQvttoh4sppdj0rL3s59kA2w2Lybr7q45wEXA78ja92sIktuW/Pvkj4k62a7l2yAwedT9yBkIwrvIxsI8CbZB/4lOfu7hOwDdWl6fsvIPoAhG3AyT9Jq4FfAyHTebPO415G12l4BppElj3+Sdds9W03MW3v990xlq/h0dOEv0rJzgIWpW/BCsvM19eEsssEabwGPAFdt1iU4mWw056oUw+npHNB6soR0EtlgmNuAcyPila3s606y82jvSXq0muU/IRs5OYdsBOPzqcwKkGo+F21m9Sm1sN4j63p7s7HjKQSSriYb0fn1xo7FCo9bUGZ5JOnLqRu0Jdkw85fIhj2b2TY4QZnl13A+PeHfm6wbz90WZrXgLj4zMytIbkFtJ0mXKbta8lxJD0gqyll2azoBXjk/WtLydHHN2ZIuSOX7SHo+lc2TdGFjPBczs0KyU7agOnbsGD169Mj7ftavX8+rr75Kv379aNKkCW+88QZ77rknHTt25KOPPmLZsmW89957HHzwwQCsWLGCNWvWsPfee2+ynU8+yUbzNmnShI0bNzJ//nz69OlDixYt8v4czMwa26xZs1ZERKctFjTWNZby+TjkkEOiISxZsiSKi4tj5cqVsWHDhjjllFNi6tSpUVFREUOGDIm33norWrZsWVX/7rvvjosvvnir21yxYkV07949ysvL6z3em266Kfr27Rv9+vWLkSNHxscff1y17JJLLtkk1rVr18aZZ54Zn/vc5+Kwww6LN998syq+IUOGRMuWLbf5XMzMagMoDV+Lr35169aN733ve+y9997stddetGnThhNOOIFf//rXDBs2jL322vLqNH/4wx848MADGTFiBIsXf3qVlsWLF3PggQfSvXt3rrjiCrp27brFutujvLycW2+9ldLSUubOncvGjRuZODH7vWNpaSmrVq3apP6dd95Ju3btKCsr47LLLuOKK64AoKioiGuvvZYbb7yxXuMzM9ucE9R2WLVqFZMnT+bNN9/krbfe4qOPPuLee+/lwQcf5JJLLtmi/pe//GUWLlzInDlz+OIXv8ioUZ/+trR79+7MmTOHsrIyJkyYwDvvvFPv8VZUVPDxxx9TUVHBmjVr6Nq1Kxs3buTyyy/nhhtu2KTu5MmTq+IbMWIE06dPJyJo2bIlRx99NEVFRdXtwsys3jhBbYe//OUv9OzZk06dOtG8eXNOP/10rrrqKsrKyujVqxc9evRgzZo19OqV3Ty2Q4cO7LbbbgBccMEFzJo1a4ttdu3alf79+/P3v/99i2Xbo66tvfLycrp3zy6R1qxZM9q0acPKlZ/luqpmZp9Ng93tcme0995788wzz7BmzRp23313pk+fzrhx4zZpPbVq1YqysjIAli5dWpUIpkyZwgEHHADAkiVL6NChA7vvvjurVq1i5syZXHbZZfUaa25rr23btnzlK1+pau3NmDGjXvdlZtu2YcMGlixZwtq1tblLyM6hqKiI4uJimjdvXqv6TlDb4fDDD2fEiBEMGjSIZs2acfDBBzN27Nga6996661MmTKFZs2a0b59e+655x4AXn75Zb773e8iiYjge9/7HgMGDKjXWHNbe0BVa+/jjz+uauFVtvbKysro1q0bixcvpri4mIqKCt5//306dKjThb/NbCuWLFlC69at6dGjB9l1knduEcHKlStZsmQJPXv2rNU6TlDb6ZprruGaa66pcfnq1VU/g+K6667juuuu26LOF7/4RebMmZOX+CrVtbU3bNgwJkyYwJFHHslDDz3Ecccdt0u8icwaytq1a3eZ5AQgiQ4dOrB8+fJar+MEtYuoa2tvzJgxnHPOOfTq1Yv27dtXjfgD6NGjBx988AHr16/n0Ucf5YknnqBv374N8TTMdiq7SnKqVNfnu1P+ULekpCRKS0sbOwwzsxq9/PLLVeehdyXVPW9JsyKiZPO6bkGZmRWAHlc+Vq/bW3j9KbWq9/bbb3PppZfy3HPP0bZtW7p06cItt9zCfvvtt0Xd9957j9/97ndcdNFF9RprTZygalDf/yz5VNt/RDOzXBHBaaedxqhRo6q68V988UXeeeedGhPUbbfd1mAJyr+DMjPbRT355JM0b96cCy/89PrUBx10EAcffDDHH388gwYNYsCAAUyePBmAK6+8ktdff52BAwdy+eWXs3TpUgYPHszAgQPz8vtNt6B2Ble3aewIau/q9xs7AjNL5s6dyyGHHLJFeVFREY888gh77rknK1as4IgjjmDYsGFcf/31zJ07l9mzZwPwy1/+kqFDh/LDH/6QjRs3smbNmnqNzwnKzMw2ERH84Ac/4KmnnqJJkyaUl5dXe/m1Qw89lPPPP58NGzZw6qmnMnDgwHqNw118Zma7qH79+lV7ybX777+f5cuXM2vWLGbPnk2XLl2qveLF4MGDeeqpp+jWrRujR4/m3nvvrdf4nKDMzHZRxx13HOvWrWP8+PFVZXPmzGHRokV07tyZ5s2b8+STT7Jo0SIAWrduzYcfflhVd9GiRXTp0oVvfOMbXHDBBTz//PP1Gp+7+MzMCkBjjMaVxCOPPMKll17Kz3/+c4qKiujRowdXX3013/72txkwYAAlJSXsv//+QHbB66OOOor+/ftz0kkn0b9/f37xi1/QvHlzWrVqVe8tKCcoM7NdWNeuXZk0adIW5U8//XS19X/3u99tMp9726D65i4+MzMrSHlNUJIukzRP0lxJD0gqktRT0rOSyiT9XlKLVHe3NF+WlvfI2c73U/mrkobmM2YzMysMeUtQkroB3wZKIqI/0BQYCfwcuDkiegGrgDFplTHAqlR+c6qHpL5pvX7AicBtkprmK24zMysM+e7iawbsLqkZsAewFDgOeCgtnwCcmqaHp3nS8uOVXfp2ODAxItZFxJtAGXBYnuM2M7NGlrcEFRHlwI3Av8gS0/vALOC9iKhI1ZYA3dJ0N2BxWrci1e+QW17NOlUkjZVUKqm0LvcbMTOzwpTPLr52ZK2fnkBXoCVZF11eRMT4iCiJiJLKu8aamdmOK5/DzL8AvBkRywEkPQwcBbSV1Cy1koqB8lS/HOgOLEldgm2AlTnllXLXMTPbOdT3NTVrcd3Lpk2bMmDAADZs2ECzZs0499xzueyyy2jSpDAGeOczin8BR0jaI51LOh6YDzwJjEh1RgGT0/SUNE9a/tfI7qY4BRiZRvn1BHoD/8xj3GZmu4Tdd9+d2bNnM2/ePKZNm8af//xnrrnmmsYOq0o+z0E9SzbY4XngpbSv8cAVwDhJZWTnmO5Mq9wJdEjl44Ar03bmAZPIktvjwMURsTFfcZuZ7Yo6d+7M+PHj+fWvf01EsHbtWs477zwGDBjAwQcfzJNPPgnAmjVrOPPMM+nbty+nnXYahx9+OKWlpWzcuJHRo0fTv39/BgwYwM0337zdMeX1ShIRcRVw1WbFb1DNKLyIWAt8pYbt/BT4ab0HaGZmVfbdd182btzIsmXL+O1vf4skXnrpJV555RVOOOEEFixYwG233Ua7du2YP38+c+fOrbqC+ezZsykvL2fu3LlAdnPD7VUYHY1mZlZQZs6cyde//nUA9t9/f/bZZx8WLFjAzJkzGTlyJAD9+/fnwAMPBLLk9sYbb3DJJZfw+OOPs+eee253DE5QZmYGwBtvvEHTpk3p3Llznddt164dL774IkOGDOGOO+7gggsu2O54nKDMzIzly5dz4YUX8q1vfQtJHHPMMdx///0ALFiwgH/961/06dOHo446qurisvPnz+ell14CYMWKFXzyySecccYZ/OQnP6mXW2/4auZmZoWgFsPC69vHH3/MwIEDq4aZn3POOYwbNw6Aiy66iG9+85sMGDCAZs2acc8997Dbbrtx0UUXMWrUKPr27cv+++9Pv379aNOmDeXl5Zx33nl88sknAFx33XXbHZ8TlJnZLmrjxpoHRBcVFXH33XdXW/7b3/6WoqIiXn/9db7whS+wzz770KJFC9+w0MzMGs+aNWs49thj2bBhAxHBbbfdRosWLfKyLycoMzOrtdatW1NaWtog+/IgCTOzRpJdLGfXUdfn6wRlZtYIioqKWLly5S6TpCKClStXUlRUVOt13MVnZtYIiouLWbJkCbvS7YGKioooLi6udX0nKDOzRtC8eXN69uzZ2GEUNHfxmZlZQXKCMjOzguQEZWZmBckJyszMCpITlJmZFSQnKDMzK0h5S1CS+kianfP4QNKlktpLmibptfS3XaovSbdKKpM0R9KgnG2NSvVfkzQqXzGbmVnhyFuCiohXI2JgRAwEDgHWAI8AVwLTI6I3MD3NA5wE9E6PscDtAJLak902/nCyW8VfVZnUzMxs59VQXXzHA69HxCJgODAhlU8ATk3Tw4F7I/MM0FbSXsBQYFpEvBsRq4BpwIkNFLeZmTWShkpQI4EH0nSXiFiapt8GuqTpbsDinHWWpLKays3MbCeW9wQlqQUwDHhw82WRXSWxXq6UKGmspFJJpbvSta3MzHZWDdGCOgl4PiLeSfPvpK470t9lqbwc6J6zXnEqq6l8ExExPiJKIqKkU6dO9fwUzMysoTVEgjqLT7v3AKYAlSPxRgGTc8rPTaP5jgDeT12BU4ETJLVLgyNOSGVmZrYTy+vVzCW1BL4I/FtO8fXAJEljgEXAman8T8DJQBnZiL/zACLiXUnXAs+lej+OiHfzGbeZmTW+vCaoiPgI6LBZ2UqyUX2b1w3g4hq2cxdwVz5iNDOzwuQrSZiZWUFygjIzs4LkBGVmZgXJCcrMzAqSE5SZmRUkJygzMytITlBmZlaQnKDMzKwgOUGZmVlBcoIyM7OC5ARlZmYFyQnKzMwKkhOUmZkVJCcoMzMrSE5QZmZWkJygzMysIDlBmZlZQXKCMjOzgpTXBCWpraSHJL0i6WVJR0pqL2mapNfS33apriTdKqlM0hxJg3K2MyrVf03SqHzGbGZmhSHfLahfAY9HxP7AQcDLwJXA9IjoDUxP8wAnAb3TYyxwO4Ck9sBVwOHAYcBVlUnNzMx2XnlLUJLaAIOBOwEiYn1EvAcMByakahOAU9P0cODeyDwDtJW0FzAUmBYR70bEKmAacGK+4jYzs8KQzxZUT2A5cLekFyT9t6SWQJeIWJrqvA10SdPdgMU56y9JZTWVb0LSWEmlkkqXL19ez0/FzMwaWj4TVDNgEHB7RBwMfMSn3XkAREQAUR87i4jxEVESESWdOnWqj02amVkjymeCWgIsiYhn0/xDZAnrndR1R/q7LC0vB7rnrF+cymoqNzOznVjeElREvA0sltQnFR0PzAemAJUj8UYBk9P0FODcNJrvCOD91BU4FThBUrs0OOKEVGZmZjuxZnne/iXA/ZJaAG8A55ElxUmSxgCLgDNT3T8BJwNlwJpUl4h4V9K1wHOp3o8j4t08x21mZo0srwkqImYDJdUsOr6augFcXMN27gLuqt/ozMyskPlKEmZmVpCcoMzMrCA5QZmZWUFygjIzs4LkBGVmZgXJCcrMzAqSE5SZmRUkJygzMytITlBmZlaQnKDMzKwgOUGZmVlBcoIyM7OC5ARlZmYFyQnKzMwKkhOUmZkVJCcoMzMrSE5QZmZWkPKaoCQtlPSSpNmSSlNZe0nTJL2W/rZL5ZJ0q6QySXMkDcrZzqhU/zVJo/IZs5mZFYaGaEEdGxEDI6Ly1u9XAtMjojcwPc0DnAT0To+xwO2QJTTgKuBw4DDgqsqkZmZmO6/G6OIbDkxI0xOAU3PK743MM0BbSXsBQ4FpEfFuRKwCpgEnNnTQZmbWsPKdoAJ4QtIsSWNTWZeIWJqm3wa6pOluwOKcdZeksprKNyFprKRSSaXLly+vz+dgZmaNoFmet390RJRL6gxMk/RK7sKICElRHzuKiPHAeICSkpJ62aaZmTWeWrWgJB1Vm7LNRUR5+rsMeITsHNI7qeuO9HdZql4OdM9ZvTiV1VRuZmY7sdp28f1/tSyrIqmlpNaV08AJwFxgClA5Em8UMDlNTwHOTaP5jgDeT12BU4ETJLVLgyNOSGVmZrYT22oXn6Qjgc8DnSSNy1m0J9B0G9vuAjwiqXI/v4uIxyU9B0ySNAZYBJyZ6v8JOBkoA9YA5wFExLuSrgWeS/V+HBHv1vL5mZnZDmpb56BaAK1SvdY55R8AI7a2YkS8ARxUTflK4PhqygO4uIZt3QXctY1YzcxsJ7LVBBURfwP+JumeiFjUQDGZmZnVehTfbpLGAz1y14mI4/IRlJmZWW0T1IPAHcB/AxvzF46ZmVmmtgmqIiJuz2skZmZmOWo7zPx/JF0kaa90sdf26Rp5ZmZmeVHbFlTl75YuzykLYN/6DcfMzCxTqwQVET3zHYiZmVmuWiUoSedWVx4R99ZvOGZmZpnadvEdmjNdRPZD2+cBJygzM8uL2nbxXZI7L6ktMDEvEZmZmfHZ7wf1EeDzUmZmlje1PQf1P2Sj9iC7SOwBwKR8BWVmZlbbc1A35kxXAIsiYkke4jEzMwNq2cWXLhr7CtkVzdsB6/MZlJmZWW3vqHsm8E/gK2T3b3pW0lZvt2FmZrY9atvF90Pg0HTrdiR1Av4CPJSvwMzMbNdW21F8TSqTU7KyDuuamZnVWW2TzOOSpkoaLWk08BjZLdq3SVJTSS9I+mOa7ynpWUllkn4vqUUq3y3Nl6XlPXK28f1U/qqkoXV5gmZmtmPaaoKS1EvSURFxOfBfwIHp8TQwvpb7+A7wcs78z4GbI6IXsAoYk8rHAKtS+c2pHpL6AiOBfsCJwG2SmtZy32ZmtoPaVgvqFuADgIh4OCLGRcQ44JG0bKskFQOnkN3oEEkCjuPTc1cTgFPT9PA0T1p+fKo/HJgYEesi4k2gDDisdk/PzMx2VNtKUF0i4qXNC1NZj1ps/xbg34FP0nwH4L2IqEjzS4BuabobsDhtvwJ4P9WvKq9mnSqSxkoqlVS6fPnyWoRmZmaFbFsJqu1Wlu2+tRUlfQlYFhGz6hzVZxAR4yOiJCJKOnXq1BC7NDOzPNpWgiqV9I3NCyVdAGwr8RwFDJO0kOzCsscBvwLaSqoc3l4MlKfpcqB72n4zoA3ZaMGq8mrWMTOzndS2EtSlwHmSZkj6ZXr8jWxAw3e2tmJEfD8iiiOiB9kgh79GxNnAk0Dlj3xHAZPT9BQ+vXPviFQ/UvnINMqvJ9Cb7EfDZma2E9vqD3Uj4h3g85KOBfqn4sci4q/bsc8rgImSfgK8ANyZyu8E7pNUBrxLltSIiHmSJgHzya4DeHFEbNyO/ZuZ2Q6gtveDepKs5fOZRMQMYEaafoNqRuFFxFqySylVt/5PgZ9+1v2bmdmOx1eDMDOzguQEZWZmBckJyszMCpITlJmZFSQnKDMzK0hOUGZmVpCcoMzMrCA5QZmZWUFygjIzs4LkBGVmZgXJCcrMzAqSE5SZmRUkJygzMytITlBmZlaQnKDMzKwgOUGZmVlBcoIyM7OClLcEJalI0j8lvShpnqRrUnlPSc9KKpP0e0ktUvluab4sLe+Rs63vp/JXJQ3NV8xmZlY48tmCWgccFxEHAQOBEyUdAfwcuDkiegGrgDGp/hhgVSq/OdVDUl9gJNAPOBG4TVLTPMZtZmYFIG8JKjKr02zz9AjgOOChVD4BODVND0/zpOXHS1IqnxgR6yLiTaAMOCxfcZuZWWHI6zkoSU0lzQaWAdOA14H3IqIiVVkCdEvT3YDFAGn5+0CH3PJq1snd11hJpZJKly9fno+nY2ZmDSivCSoiNkbEQKCYrNWzfx73NT4iSiKipFOnTvnajZmZNZAGGcUXEe8BTwJHAm0lNUuLioHyNF0OdAdIy9sAK3PLq1nHzMx2UvkcxddJUts0vTvwReBlskQ1IlUbBUxO01PSPGn5XyMiUvnINMqvJ9Ab+Ge+4jYzs8LQbNtVPrO9gAlpxF0TYFJE/FHSfGCipJ8ALwB3pvp3AvdJKgPeJRu5R0TMkzQJmA9UABdHxMY8xm1mZgUgbwkqIuYAB1dT/gbVjMKLiLXAV2rY1k+Bn9Z3jGZmVrh8JQkzMytITlBmZlaQnKDMzKwgOUGZmVlBcoIyM7OC5ARlVguLFy/m2GOPpW/fvvTr149f/epXADz44IP069ePJk2aUFpaWlV/5cqVHHvssbRq1Ypvfetbm2zrxBNP5KCDDqJfv35ceOGFbNzoX02YVccJyqwWmjVrxi9/+Uvmz5/PM888w29+8xvmz59P//79efjhhxk8ePAm9YuKirj22mu58cYbt9jWpEmTePHFF5k7dy7Lly/nwQcfrNdY65pMAa677jp69epFnz59mDp1alX5+eefT+fOnenfv3+9xmhWG05QZrWw1157MWjQIABat27NAQccQHl5OQcccAB9+vTZon7Lli05+uijKSoq2mLZnnvuCUBFRQXr168nu2h//alrMp0/fz4TJ05k3myndk4AAA6ySURBVLx5PP7441x00UVVrbrRo0fz+OOP12t8ZrXlBGVWRwsXLuSFF17g8MMP/8zbGDp0KJ07d6Z169aMGDFi2yvUQV2T6eTJkxk5ciS77bYbPXv2pFevXvzzn9nVxAYPHkz79u3rNT6z2nKCMquD1atXc8YZZ3DLLbdUtYQ+i6lTp7J06VLWrVvHX//613qMcFO1Sabl5eV07/7p9ZiLi4spL/f1mK3xOUGZ1dKGDRs444wzOPvsszn99NO3e3tFRUUMHz6cyZMnb7vyZ1BfydSssThBmdVCRDBmzBgOOOAAxo0b95m3s3r1apYuXQpk56Aee+wx9t+//m+TVpdk2q1bNxYv/vSeoEuWLKFbty3uCWrW4PJ5NXOzncY//vEP7rvvPgYMGMDAgQMB+NnPfsa6deu45JJLWL58OaeccgoDBw6sGgXXo0cPPvjgA9avX8+jjz7KE088QYcOHRg2bBjr1q3jk08+4dhjj+XCCy+s11jrmkyHDRvG1772NcaNG8dbb73Fa6+9xmGHbXE9Z7MG5wRlVgtHH3002e3JtnTaaadVW75w4cJqy5977rn6CqtadU2m/fr148wzz6Rv3740a9aM3/zmNzRt2hSAs846ixkzZrBixQqKi4u55pprGDNmTF7jN6ukmt50O7KSkpLY/HceddXjysfqKZr8W1j0tcYOofaufr+xIzCzAiNpVkSUbF7uc1BmZlaQnKDMzKwg5S1BSeou6UlJ8yXNk/SdVN5e0jRJr6W/7VK5JN0qqUzSHEmDcrY1KtV/TdKofMVsZmaFI5+DJCqA70bE85JaA7MkTQNGA9Mj4npJVwJXAlcAJwG90+Nw4HbgcEntgauAEiDSdqZExKo8xm67kB3lfKPPNdquJm8tqIhYGhHPp+kPgZeBbsBwYEKqNgE4NU0PB+6NzDNAW0l7AUOBaRHxbkpK04AT8xW3mZkVhgY5ByWpB3Aw8CzQJSKWpkVvA13SdDdgcc5qS1JZTeVmZrYTy3uCktQK+ANwaUR8kLsssjHu9TLOXdJYSaWSSpcvX14fmzQzs0aU1wQlqTlZcro/Ih5Oxe+krjvS32WpvBzonrN6cSqrqXwTETE+IkoioqRTp071+0TMzKzB5XMUn4A7gZcj4qacRVOAypF4o4DJOeXnptF8RwDvp67AqcAJktqlEX8npDIzM9uJ5XMU31HAOcBLkmansh8A1wOTJI0BFgFnpmV/Ak4GyoA1wHkAEfGupGuByuvD/Dgi3s1j3GZmVgDylqAiYiZQ061Cj6+mfgAX17Ctu4C76i86MzMrdL6ShJmZFSQnKDMzK0hOUGZmVpCcoMzMrCA5QZmZ5Tj//PPp3Lkz/fv3ryp78MEH6devH02aNCH3XnP3338/AwcOrHo0adKE2bNnb7K9YcOGbbKtxooVYM6cORx55JH069ePAQMGsHbtWj788MNNnkPHjh259NJL8xJvXTlBmZnlGD16NI8//vgmZf379+fhhx9m8ODBm5SfffbZzJ49m9mzZ3PffffRs2fPqrsYAzz88MO0atWqIGKtqKjg61//OnfccQfz5s1jxowZNG/enNatW1c9h9mzZ7PPPvtw+umn5y3munCCMjPLMXjwYNq3b79J2QEHHECfPn22ut4DDzzAyJEjq+ZXr17NTTfdxI9+9KO8xAl1i/WJJ57gwAMP5KCDDgKgQ4cONG3adJM6CxYsYNmyZRxzzDF5i7kunKDMzOrB73//e84666yq+f/4j//gu9/9LnvssUcjRvWpBQsWIImhQ4cyaNAgbrjhhi3qTJw4ka9+9atkFwJqfE5QZmbb6dlnn2WPPfaoOhc0e/ZsXn/9dU477bRGjuxTFRUVzJw5k/vvv5+ZM2fyyCOPMH369E3qTJw4cZMk29icoMzMttPmH+xPP/00paWl9OjRg6OPPpoFCxYwZMiQxgsQKC4uZvDgwXTs2JE99tiDk08+meeff75q+YsvvkhFRQWHHHJII0a5KScoM7Pt8MknnzBp0qRNzj9985vf5K233mLhwoXMnDmT/fbbjxkzZjRekMDQoUN56aWXWLNmDRUVFfztb3+jb9++VcsfeOCBgmo9gROUmdkmzjrrLI488kheffVViouLufPOO3nkkUcoLi7m6aef5pRTTmHo0KFV9Z966im6d+/OvvvuW9CxtmvXjnHjxnHooYcycOBABg0axCmnnFK1rUmTJhVcglJ2jdadS0lJSWw+/r+uelz5WD1Fk38Li77W2CHU3tXvN3YEW9hRjrWPs+2sJM2KiJLNy92CMjOzguQEZWZmBckJyszMClI+76hrZlYwdpRzjeDzjZXy1oKSdJekZZLm5pS1lzRN0mvpb7tULkm3SiqTNEfSoJx1RqX6r0kala94zcyssOSzi+8e4MTNyq4EpkdEb2B6mgc4CeidHmOB2yFLaMBVwOHAYcBVlUnNzMx2bnlLUBHxFPDuZsXDgQlpegJwak75vZF5BmgraS9gKDAtIt6NiFXANLZMemZmthNq6EESXSJiaZp+G+iSprsBi3PqLUllNZWbmdlOrtFG8UX2C+F6+5WwpLGSSiWVLl++vL42a2ZmjaShE9Q7qeuO9HdZKi8HuufUK05lNZVvISLGR0RJRJR06tSp3gM3M7OG1dAJagpQORJvFDA5p/zcNJrvCOD91BU4FThBUrs0OOKEVGZmZju5vP0OStIDwBCgo6QlZKPxrgcmSRoDLALOTNX/BJwMlAFrgPMAIuJdSdcCz6V6P46IzQdemJnZTihvCSoiaros7vHV1A3g4hq2cxdwVz2GZmZmOwBf6sjMzAqSE5SZmRUkJygzMytITlBmZlaQnKDMzKwgOUGZmVlBcoIyM7OC5ARlZmYFyQnKzMwKkhOUmZkVJCcoMzMrSE5QZmZWkJygzMysIDlBmZlZQXKCMjOzguQEZWZmBckJyszMCpITlJmZFaQdJkFJOlHSq5LKJF3Z2PGYmVl+7RAJSlJT4DfASUBf4CxJfRs3KjMzy6cdIkEBhwFlEfFGRKwHJgLDGzkmMzPLo2aNHUAtdQMW58wvAQ7PrSBpLDA2za6W9GoDxdboBB2BFY0dR61co8aOYIfl47zr2AWP9T7VFe4oCWqbImI8ML6x42gMkkojoqSx47D88nHedfhYZ3aULr5yoHvOfHEqMzOzndSOkqCeA3pL6impBTASmNLIMZmZWR7tEF18EVEh6VvAVKApcFdEzGvksArJLtm1uQvycd51+FgDiojGjsHMzGwLO0oXn5mZ7WKcoMzMrCA5QZmZWUFygmogkjpImp0eb0sqz5lvsY11SyTd+hn3+0NJ8yTNSfs6fBv1R0vq+ln2ZdWT9KSkoZuVXSrp9hrqz5BU429gJJ0v6aV0TOdK2upVVSSd6kuD5df2vL/T+kMkfX4bdfqk/43Zkl6WtNWBFJJ6SPpaXZ9LIdkhRvHtDCJiJTAQQNLVwOqIuLFyuaRmEVFRw7qlQGld9ynpSOBLwKCIWCepI7CtN8toYC7wVl33ZzV6gOynEVNzykYC/17XDUkqBn5Idkzfl9QK6LSN1U4F/gjMr+v+rHa29f6uhSHAauB/t1LnVuDmiJic9jNgG9vsAXwN+F0d4igobkE1Ikn3SLpD0rPADZIOk/S0pBck/a+kPqneEEl/TNNXS7orfZN6Q9K3t7KLvYAVEbEOICJWRMRbaTuHSPqbpFmSpkraS9IIoAS4P31L2z2vL8Cu4yHglMpv0pJ6AF3JLnpcmlq419RyW52BD8k+zIiI1RHxZtru5yQ9no7p3yXtn76VDwN+kY7p5+r3qVlNqnuPpfJvS5qfWsAT0//DhcBl6RgdU8Mm9yK7zBsAEfFS2l5TSb+Q9Fza5r+lKtcDx6RtXpav55lXEeFHAz+Aq4HvAfeQfbNtmsr3BJql6S8Af0jTQ4A/5qz7v8BuZNfrWgk0r2E/rYDZwALgNuD/pvLmaRud0vxXyX5bBjADKGns12hne6TjPDxNXwncCLRP803T637gto5BqjsV+BdwN/DlnGXTgd5p+nDgr2n6HmBEY78Gu8ojvUcv38p77C1gtzTdNmed721ju+cB7wN/Bi7LWXcs8KM0vRtZb0vP3M+NHfXhLr7G92BEbEzTbYAJknoDQZZIqvNYZK2idZKWAV3I+WZVKSJWSzoEOAY4Fvh9updWKdAfmCYJsg+9pfX4nGxLld18k9PfMcCZ6SLHzci+HfcF5mxtIxGxUdKJwKHA8cDN6RjfCHweeDAdU8g+rKxx7EbN77E5ZL0UjwKP1naDEXG3pKnAiWR3c/g3SQcBJwAHph4QyD5HegPr6+OJNCYnqMb3Uc70tcCTEXFaavbPqGGddTnTG9nKcUzJbwYwQ9JLwChgFjAvIo78zFFbXU0mSyaDgD2Ad8la0YdGxCpJ9wBFtdlQZF+V/wn8U9I0spbUTcB7ETEwH8FbnYma32OnAIOBLwM/rMW5pCqRddHfBdwlaS5ZEhRwSUTknuNE0pDPGHvB8DmowtKGTy+CO3p7N5ZG/fTOKRoILAJeBTqlQRRIai6pX6rzIdB6e/dtm4qI1cCTZB8uD5B1534EvC+pC9nNOLdJUteU5CoNBBZFxAfAm5K+kuopfbsGH9PGsI5q3mOSmgDdI+JJ4Aqy93wranGMlN1VvHma/j9AB7LPi6nAN3OW7SepZW22WeicoArLDcB1kl6gflq3rci6DOdLmkPWhXR1ZDd9HAH8XNKLZOepKoe43gPc4UESefEAcBDwQES8CLwAvEI2yuoftdxGc+BGSa9Imk12buM7adnZwJh0TOfx6U09JwKXp8E3HiTRMD6h+vdYU+C3qTfjBeDWiHgP+B/gtG0MkjgBmJu2NxW4PCLeBv6bbITm86lV9V9knx9zgI2SXtxRB0n4WnxmZlaQ3IIyM7OC5EESOwFJHciGGG/u+Mh+QGg7oPT7uM1H4p0T6fcvtnOS9EPgK5sVPxgRP22MeBqTu/jMzKwguYvPzMwKkhOUmZkVJCcoMzMrSE5QZmZWkP5/778UNge48ZEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#get class proportion in each set\n",
        "train_cats_count = len([el for el in os.listdir(train_path) if el[0]=='0'])\n",
        "train_dogs_count = len([el for el in os.listdir(train_path) if el[0]=='1'])\n",
        "val_cats_count = len([el for el in os.listdir(val_path) if el[0]=='0'])\n",
        "val_dogs_count = len([el for el in os.listdir(val_path) if el[0]=='1'])\n",
        "test_cats_count = len([el for el in os.listdir(test_path) if el[0]=='0'])\n",
        "test_dogs_count = len([el for el in os.listdir(test_path) if el[0]=='1'])\n",
        "\n",
        "#plot\n",
        "labels = ['Train_Set', 'Val_Set', 'Test_Set']\n",
        "cats = [train_cats_count, val_cats_count, test_cats_count]\n",
        "dogs = [train_dogs_count, val_dogs_count, test_dogs_count]\n",
        "\n",
        "label = np.array(labels)\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, cats, width, label='Cats')\n",
        "rects2 = ax.bar(x + width/2, dogs, width, label='Dogs')\n",
        "\n",
        "# Add text\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Cats And Dogs Class Proportion')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327QiNdMcUKC"
      },
      "source": [
        "##**Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKokrDGCaxg0"
      },
      "source": [
        "###Primary Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ewKNTHQ0bt8w"
      },
      "outputs": [],
      "source": [
        "#model builder\n",
        "def CnnModel(architecture):\n",
        "  '''\n",
        "  Input: Model architecture with the following specifications:\n",
        "          - input_shape (tuple of the form (w, h, channels))\n",
        "          - out_channel (integer)\n",
        "            n.b. these are used in the first CN block and are doubled for each subsequent block\n",
        "          - conv_layers (integer number of convolutional layers)\n",
        "          - fc_layers (integer number of dense layer)\n",
        "          - dropout (either True or False)\n",
        "\n",
        "  '''\n",
        "  #model parameters\n",
        "  input_shape, out_channels, conv_layers, fc_layers, dropout = architecture\n",
        "\n",
        "  #build model\n",
        "  cnn_model = Sequential([Conv2D(out_channels, 3, activation='relu', input_shape=input_shape),\n",
        "                          BatchNormalization(),\n",
        "                          MaxPooling2D(pool_size=(2,2))\n",
        "                          ])\n",
        "  if dropout:\n",
        "    cnn_model.add(Dropout(0.25))\n",
        "\n",
        "  if conv_layers > 1:\n",
        "    CL = 1\n",
        "    nth_out_channels = out_channels\n",
        "    while CL <= conv_layers-1:\n",
        "      nth_out_channels = nth_out_channels *2\n",
        "      conv_block = Sequential([Conv2D(nth_out_channels, 3,activation='relu'),\n",
        "                               BatchNormalization(),\n",
        "                               MaxPooling2D(pool_size=(2,2))\n",
        "                              ])\n",
        "      if dropout:\n",
        "        conv_block.add(Dropout(0.25))\n",
        "      for layer in conv_block.layers:\n",
        "        cnn_model.add(layer)\n",
        "      CL+=1\n",
        "    \n",
        "  if fc_layers ==1:\n",
        "    out_block = Sequential([GlobalAveragePooling2D(),\n",
        "                            Dense(2, activation='softmax')\n",
        "                            ])\n",
        "  else:\n",
        "    out_block = Sequential([Flatten()])\n",
        "    num_fc = fc_layers-1\n",
        "    output_dim = 512\n",
        "    while num_fc > 1: \n",
        "      output_dim = output_dim*2\n",
        "      num_fc-=1\n",
        "    FC = 1\n",
        "    while FC <= fc_layers-1:  \n",
        "      out_block.add(Dense(output_dim,activation='relu'))\n",
        "      out_block.add(BatchNormalization())\n",
        "      if dropout:\n",
        "        out_block.add(Dropout(0.5))\n",
        "      output_dim = int(output_dim/2)\n",
        "      FC+=1\n",
        "    out_block.add(Dense(2,activation='softmax'))\n",
        "  \n",
        "  for layer in out_block.layers:\n",
        "    cnn_model.add(layer)      \n",
        "                        \n",
        "  return cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CnnModel(((128, 128, 3), 32, 3, 3, True)).summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IZUpuiF1O6V",
        "outputId": "5c64a24c-bbeb-4a4d-ff54-34cf89337476"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 126, 126, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 126, 126, 32)     128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 63, 63, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 61, 61, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 30, 30, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 28, 28, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              25691136  \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,317,250\n",
            "Trainable params: 26,313,730\n",
            "Non-trainable params: 3,520\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sdXS1R0FSER6"
      },
      "outputs": [],
      "source": [
        "#data generator and loader function\n",
        "def data_generator(df, data_path, set_type, image_size, batch_size, xy = ['filename', 'class_id']):\n",
        "\n",
        "  x_col, y_col = xy\n",
        "\n",
        "  if set_type in ['val', 'test']:\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "  \n",
        "  elif set_type == 'train':\n",
        "    datagen = ImageDataGenerator(rotation_range=15,\n",
        "                                rescale=1./255,\n",
        "                                shear_range=0.1,\n",
        "                                zoom_range=0.2,\n",
        "                                horizontal_flip=True,\n",
        "                                width_shift_range=0.1,\n",
        "                                height_shift_range=0.1\n",
        "                                )\n",
        "\n",
        "  else:\n",
        "    print('Incorret specification for set_type argument: acceptable arguments are one of [train, test, val]')\n",
        "    pass\n",
        "  generator = datagen.flow_from_dataframe(df,\n",
        "                                          data_path,\n",
        "                                          x_col='filename',\n",
        "                                          y_col='class_id',\n",
        "                                          target_size=image_size,\n",
        "                                          class_mode='categorical',\n",
        "                                          batch_size=batch_size\n",
        "                                          )\n",
        "  return generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "43Le08qwb5HU"
      },
      "outputs": [],
      "source": [
        "#training function\n",
        "def train_model(model, fit_generator_params, loss='categorical_crossentropy', optimizer='rmsprop', metrics = ['accuracy'], cp_dir = 'model/'):\n",
        "  #define callbacks, learning rate, checkpoints\n",
        "  patience = fit_generator_params['epochs']//10\n",
        "  earlystop = EarlyStopping(patience = patience)\n",
        "  learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',patience = 2,verbose = 1,factor = 0.5,min_lr = 0.00001)\n",
        "  checkpoint_path = cp_dir + 'cnn' + '.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
        "  cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                save_weights_only=False,\n",
        "                                save_best_only = True,\n",
        "                                #save_freq=3*batch_size,\n",
        "                                verbose=1)\n",
        "  os.makedirs(cp_dir, exist_ok = True)\n",
        "  callbacks = [earlystop,learning_rate_reduction, cp_callback]\n",
        "  model.compile(loss=loss,\n",
        "                  optimizer=optimizer,metrics=metrics)\n",
        "  history = model.fit(\n",
        "    fit_generator_params['data'], \n",
        "    epochs=fit_generator_params['epochs'],\n",
        "    validation_data=fit_generator_params['validation_data'],\n",
        "    validation_steps=len(val_df)//fit_generator_params['batch_size'],\n",
        "    steps_per_epoch=len(train_df)//fit_generator_params['batch_size'],\n",
        "    callbacks=callbacks\n",
        "    ) \n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzPc0lEJcbVd"
      },
      "source": [
        "###Secondary Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9lkXhdB2bgFh"
      },
      "outputs": [],
      "source": [
        "#secondary functions\n",
        "def get_model_name(architecture):\n",
        "  input_shape, out_channels, conv_layers, fc_layers, dropout = architecture\n",
        "  if dropout:\n",
        "    dropout = '_dropout'\n",
        "  else:\n",
        "    dropout = ''\n",
        "  name = str(conv_layers)+'conv_' + str(fc_layers) + 'fc' + dropout\n",
        "  return name\n",
        "\n",
        "'''\n",
        "def exclude_trained_models(options, models_dir):\n",
        "  architecture_names = [get_model_name(architecture) for architecture in options]\n",
        "  trained_models = [model_name for model_name in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir,model_name))]\n",
        "  for architecture in options:\n",
        "    name = get_model_name(architecture)\n",
        "    if name in trained_models:\n",
        "      index = architecture_names.index(name)\n",
        "      options.pop(index)\n",
        "  return options\n",
        "'''\n",
        "def exclude_trained_models(options, models_dir):\n",
        "  architecture_names = [get_model_name(architecture) for architecture in options]\n",
        "  trained_models = [model_name for model_name in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir,model_name))]\n",
        "  new_models = list(set(architecture_names)-set(trained_models))\n",
        "  indexes = [architecture_names.index(model) for model in new_models]\n",
        "  options = [options[index] for index in indexes]\n",
        "  return options\n",
        "\n",
        "\n",
        "def save_plot_results(history, model_name, save_dir):\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "  #plot accuracy\n",
        "  accuracy_plot = save_dir + 'acc_'+ model_name\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.savefig(accuracy_plot, bbox_inches='tight')\n",
        "  plt.close()\n",
        "  #plot loss\n",
        "  loss_plot = save_dir + 'loss_' + model_name\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.savefig(loss_plot, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZcv5p7Zc8D5"
      },
      "source": [
        "##**Training**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ6azlaceInL"
      },
      "source": [
        "###Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH3v0bYP4O_I",
        "outputId": "63593954-cb77-4788-d554-6216e9bfa6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16854 validated image filenames belonging to 2 classes.\n",
            "Found 4214 validated image filenames belonging to 2 classes.\n",
            "Found 2341 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#input params\n",
        "BATCH_SIZE = 15\n",
        "EPOCHS=30\n",
        "#build data generators\n",
        "train_generator = data_generator(df=train_df, data_path=train_path, set_type = 'train',image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
        "val_generator = data_generator(df=val_df, data_path=val_path, set_type = 'val',image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
        "test_generator = data_generator(df=test_df, data_path=test_path, set_type = 'test',image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
        "\n",
        "fit_generator_params = {}\n",
        "fit_generator_params['data'] = train_generator\n",
        "fit_generator_params['validation_data'] = val_generator\n",
        "fit_generator_params['epochs'] = EPOCHS\n",
        "fit_generator_params['batch_size'] = BATCH_SIZE\n",
        "fit_generator_params['validation_steps'] = len(val_df)//BATCH_SIZE\n",
        "fit_generator_params['steps_per_epoch'] = len(train_df)//BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EUfwee-eL7N"
      },
      "source": [
        "###Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1K95gFDK4LFV"
      },
      "outputs": [],
      "source": [
        "#define grid of parameters for model\n",
        "input_shape = [IMAGE_SHAPE]\n",
        "out_channels = [32]\n",
        "conv_layers = [1,2,3,4]\n",
        "fc_layers = [1,2,3,4]\n",
        "dropout = [True, False]\n",
        "grid = [input_shape, out_channels, conv_layers, fc_layers, dropout]\n",
        "options = list(itertools.product(*grid))\n",
        "all_options = [comb for comb in options if comb[2] >= comb[3]]\n",
        "#update model dirs\n",
        "delta_options = exclude_trained_models(all_options, model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOPxMokrzRye",
        "outputId": "6f1cdb51-64ab-478e-aa30-6f9132d1906b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((128, 128, 3), 32, 4, 3, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "delta_options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GXdTuG6Yr6h",
        "outputId": "58dba248-da14-49e8-ed4a-b6e9b3c2c999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training starting for model 4conv_3fc\n",
            "Epoch 1/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.7200 - accuracy: 0.6170\n",
            "Epoch 1: val_loss improved from inf to 0.66996, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.01-0.67.hdf5\n",
            "1123/1123 [==============================] - 147s 120ms/step - loss: 0.7200 - accuracy: 0.6170 - val_loss: 0.6700 - val_accuracy: 0.6486 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.5598 - accuracy: 0.7233\n",
            "Epoch 2: val_loss improved from 0.66996 to 0.57901, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.02-0.58.hdf5\n",
            "1123/1123 [==============================] - 124s 111ms/step - loss: 0.5598 - accuracy: 0.7233 - val_loss: 0.5790 - val_accuracy: 0.7164 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.7701\n",
            "Epoch 3: val_loss improved from 0.57901 to 0.45100, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.03-0.45.hdf5\n",
            "1123/1123 [==============================] - 124s 110ms/step - loss: 0.4833 - accuracy: 0.7701 - val_loss: 0.4510 - val_accuracy: 0.7950 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8149\n",
            "Epoch 4: val_loss did not improve from 0.45100\n",
            "1123/1123 [==============================] - 123s 110ms/step - loss: 0.4187 - accuracy: 0.8149 - val_loss: 0.4862 - val_accuracy: 0.7819 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.8332\n",
            "Epoch 5: val_loss improved from 0.45100 to 0.39819, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.05-0.40.hdf5\n",
            "1123/1123 [==============================] - 124s 111ms/step - loss: 0.3751 - accuracy: 0.8332 - val_loss: 0.3982 - val_accuracy: 0.8255 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.8576\n",
            "Epoch 6: val_loss improved from 0.39819 to 0.31908, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.06-0.32.hdf5\n",
            "1123/1123 [==============================] - 127s 113ms/step - loss: 0.3359 - accuracy: 0.8576 - val_loss: 0.3191 - val_accuracy: 0.8636 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8705\n",
            "Epoch 7: val_loss did not improve from 0.31908\n",
            "1123/1123 [==============================] - 123s 110ms/step - loss: 0.3101 - accuracy: 0.8705 - val_loss: 0.3808 - val_accuracy: 0.8233 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8768\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 8: val_loss improved from 0.31908 to 0.31792, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.08-0.32.hdf5\n",
            "1123/1123 [==============================] - 123s 110ms/step - loss: 0.2942 - accuracy: 0.8768 - val_loss: 0.3179 - val_accuracy: 0.8543 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.8963\n",
            "Epoch 9: val_loss improved from 0.31792 to 0.20514, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.09-0.21.hdf5\n",
            "1123/1123 [==============================] - 121s 108ms/step - loss: 0.2518 - accuracy: 0.8963 - val_loss: 0.2051 - val_accuracy: 0.9200 - lr: 5.0000e-04\n",
            "Epoch 10/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9043\n",
            "Epoch 10: val_loss did not improve from 0.20514\n",
            "1123/1123 [==============================] - 120s 107ms/step - loss: 0.2321 - accuracy: 0.9043 - val_loss: 0.2286 - val_accuracy: 0.9131 - lr: 5.0000e-04\n",
            "Epoch 11/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9055\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.20514\n",
            "1123/1123 [==============================] - 120s 107ms/step - loss: 0.2321 - accuracy: 0.9055 - val_loss: 0.2416 - val_accuracy: 0.9026 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.9199\n",
            "Epoch 12: val_loss improved from 0.20514 to 0.19485, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.12-0.19.hdf5\n",
            "1123/1123 [==============================] - 119s 106ms/step - loss: 0.2082 - accuracy: 0.9199 - val_loss: 0.1948 - val_accuracy: 0.9243 - lr: 2.5000e-04\n",
            "Epoch 13/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9256\n",
            "Epoch 13: val_loss improved from 0.19485 to 0.16584, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.13-0.17.hdf5\n",
            "1123/1123 [==============================] - 118s 105ms/step - loss: 0.1930 - accuracy: 0.9256 - val_loss: 0.1658 - val_accuracy: 0.9336 - lr: 2.5000e-04\n",
            "Epoch 14/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9242\n",
            "Epoch 14: val_loss improved from 0.16584 to 0.15850, saving model to /content/drive/MyDrive/ML_Project/model/4conv_3fc/cnn.14-0.16.hdf5\n",
            "1123/1123 [==============================] - 119s 106ms/step - loss: 0.1906 - accuracy: 0.9242 - val_loss: 0.1585 - val_accuracy: 0.9395 - lr: 2.5000e-04\n",
            "Epoch 15/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9294\n",
            "Epoch 15: val_loss did not improve from 0.15850\n",
            "1123/1123 [==============================] - 126s 112ms/step - loss: 0.1859 - accuracy: 0.9294 - val_loss: 0.1710 - val_accuracy: 0.9321 - lr: 2.5000e-04\n",
            "Epoch 16/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.9258\n",
            "Epoch 16: val_loss did not improve from 0.15850\n",
            "1123/1123 [==============================] - 117s 104ms/step - loss: 0.1910 - accuracy: 0.9258 - val_loss: 0.1595 - val_accuracy: 0.9398 - lr: 2.5000e-04\n",
            "Epoch 17/30\n",
            "1123/1123 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.9334\n",
            "Epoch 17: val_loss did not improve from 0.15850\n",
            "1123/1123 [==============================] - 118s 105ms/step - loss: 0.1809 - accuracy: 0.9334 - val_loss: 0.1985 - val_accuracy: 0.9267 - lr: 2.5000e-04\n",
            "Training completed for model -4conv_3fc-. Weights saved at /content/drive/MyDrive/ML_Project/model/4conv_3fc/\n"
          ]
        }
      ],
      "source": [
        "#train models\n",
        "for architecture in delta_options:\n",
        "  name = get_model_name(architecture)\n",
        "  model = CnnModel(architecture)\n",
        "  cp_dir = model_dir + name +'/'\n",
        "  print(f'Training starting for model {name}')\n",
        "  history = train_model(model, fit_generator_params, cp_dir = cp_dir)\n",
        "  save_plot_results(history, name, plots_dir)\n",
        "  print(f'Training completed for model -{name}-. Weights saved at {cp_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeKB4n7wH4Dl"
      },
      "source": [
        "##**Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EPSlB8ho6gdt"
      },
      "outputs": [],
      "source": [
        "def retrieve_best_model(cp_path):\n",
        "  if os.path.isdir(cp_path):\n",
        "    model_cp = os.listdir(cp_path)\n",
        "    val_loss = [float('.'.join(name.split('-')[-1].split('.')[:2])) for name in model_cp]\n",
        "    index = val_loss.index(min(val_loss))\n",
        "    best_model = os.path.join(cp_path, model_cp[index])\n",
        "  else: \n",
        "    best_model = None\n",
        "  return best_model "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "architecture_col = []\n",
        "loss_col = []\n",
        "accuracy_col = []\n",
        "model_names = sorted(os.listdir(model_dir))\n",
        "for name in model_names:\n",
        "  cp_path = os.path.join(model_dir, name)\n",
        "  best_model = retrieve_best_model(cp_path)\n",
        "  if best_model:\n",
        "    model =  load_model(best_model)\n",
        "    loss, accuracy = model.evaluate(test_generator, verbose=0)\n",
        "    architecture_col.append(name)\n",
        "    loss_col.append(loss)\n",
        "    accuracy_col.append(accuracy)"
      ],
      "metadata": {
        "id": "JOAtNxHlW05p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zip(architecture_col, accuracy_col, loss_col)),\n",
        "               columns =['Architecture', 'Accuracy', 'Loss'])\n",
        "df.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "UnrX3lyhZE1M",
        "outputId": "cd432ef4-cdc3-414a-c1a1-a1b225490e44"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Architecture  Accuracy      Loss\n",
              "0           1conv_1fc  0.650150  0.627960\n",
              "1   1conv_1fc_dropout  0.616403  0.641317\n",
              "2           2conv_1fc  0.673644  0.627020\n",
              "3   2conv_1fc_dropout  0.753097  0.522974\n",
              "4           2conv_2fc  0.891072  0.283915\n",
              "5   2conv_2fc_dropout  0.879539  0.304324\n",
              "6           3conv_1fc  0.920974  0.208715\n",
              "7   3conv_1fc_dropout  0.738146  0.531472\n",
              "8           3conv_2fc  0.924818  0.202043\n",
              "9   3conv_2fc_dropout  0.864161  0.332816\n",
              "10          3conv_3fc  0.949167  0.145530\n",
              "11  3conv_3fc_dropout  0.665955  0.622628\n",
              "12          4conv_1fc  0.933362  0.179583\n",
              "13  4conv_1fc_dropout  0.891927  0.262291\n",
              "14          4conv_2fc  0.965827  0.090090\n",
              "15  4conv_2fc_dropout  0.946177  0.149917\n",
              "16          4conv_3fc  0.942760  0.151470\n",
              "17  4conv_3fc_dropout  0.916702  0.200192\n",
              "18          4conv_4fc  0.918411  0.187463\n",
              "19  4conv_4fc_dropout  0.650577  0.634165"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22cfd0c6-e00a-4ed9-873d-249c8bfbb0af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Architecture</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1conv_1fc</td>\n",
              "      <td>0.650150</td>\n",
              "      <td>0.627960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1conv_1fc_dropout</td>\n",
              "      <td>0.616403</td>\n",
              "      <td>0.641317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2conv_1fc</td>\n",
              "      <td>0.673644</td>\n",
              "      <td>0.627020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2conv_1fc_dropout</td>\n",
              "      <td>0.753097</td>\n",
              "      <td>0.522974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2conv_2fc</td>\n",
              "      <td>0.891072</td>\n",
              "      <td>0.283915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2conv_2fc_dropout</td>\n",
              "      <td>0.879539</td>\n",
              "      <td>0.304324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3conv_1fc</td>\n",
              "      <td>0.920974</td>\n",
              "      <td>0.208715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3conv_1fc_dropout</td>\n",
              "      <td>0.738146</td>\n",
              "      <td>0.531472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3conv_2fc</td>\n",
              "      <td>0.924818</td>\n",
              "      <td>0.202043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3conv_2fc_dropout</td>\n",
              "      <td>0.864161</td>\n",
              "      <td>0.332816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3conv_3fc</td>\n",
              "      <td>0.949167</td>\n",
              "      <td>0.145530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3conv_3fc_dropout</td>\n",
              "      <td>0.665955</td>\n",
              "      <td>0.622628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4conv_1fc</td>\n",
              "      <td>0.933362</td>\n",
              "      <td>0.179583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4conv_1fc_dropout</td>\n",
              "      <td>0.891927</td>\n",
              "      <td>0.262291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4conv_2fc</td>\n",
              "      <td>0.965827</td>\n",
              "      <td>0.090090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4conv_2fc_dropout</td>\n",
              "      <td>0.946177</td>\n",
              "      <td>0.149917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4conv_3fc</td>\n",
              "      <td>0.942760</td>\n",
              "      <td>0.151470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4conv_3fc_dropout</td>\n",
              "      <td>0.916702</td>\n",
              "      <td>0.200192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4conv_4fc</td>\n",
              "      <td>0.918411</td>\n",
              "      <td>0.187463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>4conv_4fc_dropout</td>\n",
              "      <td>0.650577</td>\n",
              "      <td>0.634165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22cfd0c6-e00a-4ed9-873d-249c8bfbb0af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22cfd0c6-e00a-4ed9-873d-249c8bfbb0af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22cfd0c6-e00a-4ed9-873d-249c8bfbb0af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_one_loss(df):\n",
        "  '''\n",
        "  Input: pandas df with True Class column and Predict Class column\n",
        "  '''\n",
        "  #cast columns\n",
        "  df['pred_class_id'] = df['pred_class_id'].astype('int')\n",
        "  test_df['class_id'] = df['class_id'].astype('int')\n",
        "  #compute loss\n",
        "  n = len(df)\n",
        "  loss_value = np.where(test_df['class_id'] != test_df['pred_class_id'], 1, 0)\n",
        "  loss = sum(loss_value)/n\n",
        "  #loss = sum(np.where(test_df['class_id'] != test_df['pred_class_id'], 1, 0))/n\n",
        "  accuracy = 1-loss\n",
        "  return loss, accuracy"
      ],
      "metadata": {
        "id": "XSPoe098SbS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hk0Uns57ck7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a295ba-ab3b-4307-cf99-9ae7d1272110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167/167 [==============================] - 25s 151ms/step\n",
            "167/167 [==============================] - 25s 150ms/step\n",
            "167/167 [==============================] - 33s 195ms/step\n",
            "167/167 [==============================] - 31s 185ms/step\n",
            "167/167 [==============================] - 38s 227ms/step\n",
            "167/167 [==============================] - 36s 212ms/step\n",
            "167/167 [==============================] - 36s 214ms/step\n",
            "167/167 [==============================] - 42s 251ms/step\n",
            "167/167 [==============================] - 44s 261ms/step\n",
            "167/167 [==============================] - 38s 227ms/step\n",
            "167/167 [==============================] - 36s 215ms/step\n",
            "167/167 [==============================] - 39s 235ms/step\n",
            "167/167 [==============================] - 41s 241ms/step\n",
            "167/167 [==============================] - 39s 235ms/step\n",
            "167/167 [==============================] - 42s 249ms/step\n",
            "167/167 [==============================] - 42s 251ms/step\n",
            "167/167 [==============================] - 47s 278ms/step\n",
            "167/167 [==============================] - 40s 240ms/step\n"
          ]
        }
      ],
      "source": [
        "model_performances = {}\n",
        "for cp in os.listdir(model_dir):\n",
        "  cp_path = os.path.join(model_dir, cp)\n",
        "  best_model = retrieve_best_model(cp_path)\n",
        "  if best_model:\n",
        "    model =  load_model(best_model)\n",
        "    model_name = best_model.split('/')[-2]\n",
        "    #predict on test\n",
        "    predict = model.predict(test_generator, steps=np.ceil(nb_samples/BATCH_SIZE))\n",
        "    test_df['pred_class_id'] = np.argmax(predict, axis=-1)\n",
        "    test_df['pred_class_id'] = test_df['pred_class_id'].astype('int')\n",
        "    test_df['class_id'] = test_df['class_id'].astype('int')\n",
        "    #compute loss\n",
        "    loss, accuracy = zero_one_loss(test_df)\n",
        "    performances = {}\n",
        "    performances['loss'] = loss\n",
        "    performances['accuracy'] = accuracy\n",
        "    model_performances[model_name] = performances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHmHsiPIvImX",
        "outputId": "c5c65669-7678-4c82-f52e-decb93f33189"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ML_Project/model/1conv_1fc/cnn.10-0.62.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/1conv_1fc_dropout/cnn.03-0.64.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/2conv_1fc/cnn.04-0.61.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/2conv_1fc_dropout/cnn.10-0.52.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/2conv_2fc/cnn.13-0.31.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/2conv_2fc_dropout/cnn.11-0.37.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/3conv_1fc/cnn.11-0.31.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/3conv_1fc_dropout/cnn.05-0.52.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/3conv_2fc/cnn.09-0.24.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/3conv_2fc_dropout/cnn.06-0.36.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/3conv_3fc_dropout/cnn.01-0.60.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/4conv_1fc/cnn.07-0.21.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/4conv_1fc_dropout/cnn.11-0.29.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/4conv_2fc_dropout/cnn.12-0.18.hdf5',\n",
              " '/content/drive/MyDrive/ML_Project/model/4conv_3fc_dropout/cnn.09-0.23.hdf5']"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#collect best model weights from each architecture\n",
        "model_weights = []\n",
        "for dir in os.listdir(model_dir):\n",
        "  dir_path = os.path.join(model_dir, dir)\n",
        "  files = [] \n",
        "  if os.path.isdir(dir_path):\n",
        "    for file in os.listdir(dir_path):\n",
        "      model_path = os.path.join(dir_path , file)\n",
        "      files.append(model_path)\n",
        "    #only append the last (best) model  \n",
        "    model_weights.append(files[-1])\n",
        "model_weights = sorted(model_weights)\n",
        "model_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnM5THhhzbem",
        "outputId": "c5ad67ea-c4c5-42fd-bf63-24f9e698ef9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'1conv_1fc': 0.4928,\n",
              " '1conv_1fc_dropout': 0.5,\n",
              " '2conv_1fc': 0.5028,\n",
              " '2conv_1fc_dropout': 0.5072,\n",
              " '2conv_2fc': 0.4852,\n",
              " '2conv_2fc_dropout': 0.498,\n",
              " '3conv_1fc': 0.5136,\n",
              " '3conv_1fc_dropout': 0.5108,\n",
              " '3conv_2fc': 0.5016,\n",
              " '3conv_2fc_dropout': 0.496,\n",
              " '3conv_3fc_dropout': 0.5048,\n",
              " '4conv_1fc': 0.5024,\n",
              " '4conv_1fc_dropout': 0.4984,\n",
              " '4conv_2fc_dropout': 0.4896,\n",
              " '4conv_3fc_dropout': 0.5028}"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SJ3jD1ogr2AO",
        "outputId": "502c36d6-a7f7-45ed-b18b-19e478d210e3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9d33a82c-8332-41f2-82c6-63b96c4c0952\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class_id</th>\n",
              "      <th>pred_class_id</th>\n",
              "      <th>pred_category</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/ML_Project/dataset/CatsDogs/test/1110...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>cat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/ML_Project/dataset/CatsDogs/test/1377...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>dog</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/ML_Project/dataset/CatsDogs/test/0390...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>cat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/ML_Project/dataset/CatsDogs/test/1899...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>dog</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/ML_Project/dataset/CatsDogs/test/1113...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>cat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d33a82c-8332-41f2-82c6-63b96c4c0952')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d33a82c-8332-41f2-82c6-63b96c4c0952 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d33a82c-8332-41f2-82c6-63b96c4c0952');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            filename  class_id  pred_class_id  \\\n",
              "0  /content/ML_Project/dataset/CatsDogs/test/1110...         1              0   \n",
              "1  /content/ML_Project/dataset/CatsDogs/test/1377...         1              1   \n",
              "2  /content/ML_Project/dataset/CatsDogs/test/0390...         0              0   \n",
              "3  /content/ML_Project/dataset/CatsDogs/test/1899...         1              1   \n",
              "4  /content/ML_Project/dataset/CatsDogs/test/1113...         1              0   \n",
              "\n",
              "  pred_category category  \n",
              "0           cat        0  \n",
              "1           dog        1  \n",
              "2           cat        0  \n",
              "3           dog        1  \n",
              "4           cat        0  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df['pred_class_id'] = np.argmax(predict, axis=-1)\n",
        "test_df['pred_category'] = np.where(test_df['pred_class_id'] == 1, 'dog', 'cat')\n",
        "test_df['pred_class_id'] = test_df['pred_class_id'].astype('int')\n",
        "test_df['class_id'] = test_df['class_id'].astype('int')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}